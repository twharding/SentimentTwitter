{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import html\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC, NuSVC \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data & test data\n",
    "\n",
    "names = ('polarity', 'id', 'date', 'query', 'author', 'text')\n",
    "data_train = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin1', names=names)\n",
    "data_test = pd.read_csv('testdata.manual.2009.06.14.csv', names=names)\n",
    "\n",
    "# sample n from 160k tweets\n",
    "data_train_sample = data_train.sample(400000)\n",
    "\n",
    "# split into X and y\n",
    "textcorpus = data_train['text']\n",
    "text_train_all = data_train_sample['text']\n",
    "target_train_all = data_train_sample['polarity'].values\n",
    "target_train_all = target_train_all / 4\n",
    "\n",
    "# split training csv into training and validation components\n",
    "\n",
    "text_train_small, text_validation, target_train_small, target_validation = train_test_split(\n",
    "    text_train_all, target_train_all, test_size=.2, random_state=42)\n",
    "\n",
    "del data_train, data_test, data_train_sample, target_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Tweet text cleaning #\n",
    "#######################\n",
    "\n",
    "# Load contractions dictionary\n",
    "contractions = pickle.load(open(\"contractions.pickle\", \"rb\"))\n",
    "\n",
    "# Expand contractions\n",
    "def contraction_remove(line):\n",
    "    for word in line.split():\n",
    "        if word.lower() in contractions:\n",
    "            line = line.replace(word, contractions[word.lower()])\n",
    "    return line\n",
    "\n",
    "# General text cleanup\n",
    "def text_cleanup(text):\n",
    "    #Remove &quot; or &amp;\n",
    "    souped = html.unescape(text)\n",
    "    #Remove @mentions\n",
    "    #souped = re.sub(r'@\\w+','',souped)\n",
    "    #Remove http / https links\n",
    "    souped = re.sub(r'https?://\\S*','',souped)\n",
    "    #Remove all remaining numbers / non-letters\n",
    "    souped = re.sub(\"[^a-zA-Z]\",' ',souped)\n",
    "    #All lower case\n",
    "    #souped = souped.lower()\n",
    "    return(souped)\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def stopword_remove(line):\n",
    "    words = [w.strip() for w in line.split() if not w.strip() in stop_words]\n",
    "    words = str.join(' ',words)\n",
    "    return words\n",
    "\n",
    "# Top level tweet cleaner function\n",
    "def tweets_clean(tweets):\n",
    "    #tweets = tweets.apply(lambda x: contraction_remove(x))\n",
    "    tweets = tweets.apply(lambda x: text_cleanup(x))\n",
    "    #tweets = tweets.apply(lambda x: stopword_remove(x))\n",
    "    return tweets\n",
    "\n",
    "text_train_small = tweets_clean(text_train_small)\n",
    "text_validation = tweets_clean(text_validation)\n",
    "text_train_all = tweets_clean(text_train_all)\n",
    "textcorpus = tweets_clean(textcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpustokens = []\n",
    "for line in textcorpus:\n",
    "    corpustokens.append(line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(corpustokens, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pleased', 0.6263509392738342),\n",
       " ('thrilled', 0.5964668393135071),\n",
       " ('thankful', 0.5762211084365845),\n",
       " ('upset', 0.5726284980773926),\n",
       " ('grateful', 0.5704963803291321),\n",
       " ('unhappy', 0.5698879361152649),\n",
       " ('excited', 0.5605750679969788),\n",
       " ('depressed', 0.5534179210662842),\n",
       " ('proud', 0.5514568090438843),\n",
       " ('exited', 0.5495133996009827)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    320000.000000\n",
       "mean         18.586037\n",
       "std           9.632124\n",
       "min           2.000000\n",
       "25%          11.000000\n",
       "50%          17.000000\n",
       "75%          25.000000\n",
       "max         356.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = text_train_small.apply(lambda x: len(x.split(' ')))\n",
    "seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 556144 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "max_words = 37 #Â Maximum length of tweet word sequence\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(textcorpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "tokencorpus = tokenizer.texts_to_sequences(text_train_small)\n",
    "x_train = tokenizer.texts_to_sequences(text_train_small)\n",
    "x_valid = tokenizer.texts_to_sequences(text_validation)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % vocab_size)\n",
    "sequences_train = pad_sequences(x_train, maxlen=max_words)\n",
    "sequences_valid = pad_sequences(x_valid, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/tom/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDINGS_LEN= 300\n"
     ]
    }
   ],
   "source": [
    "embeddings_len = len(w2v['hi'])\n",
    "print(\"EMBEDDINGS_LEN=\", embeddings_len)  # 300\n",
    " \n",
    "embeddings_matrix = np.zeros((vocab_size, embeddings_len))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding = w2v[word]\n",
    "        embeddings_matrix[idx] = embedding\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            embeddings_len,\n",
    "                            weights=[embeddings_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tom/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         166843200 \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 300)               540900    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 167,384,401\n",
      "Trainable params: 541,201\n",
      "Non-trainable params: 166,843,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(300))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tom/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 320000 samples, validate on 80000 samples\n",
      "Epoch 1/1\n",
      "320000/320000 [==============================] - 3241s 10ms/step - loss: 0.4358 - acc: 0.7961 - val_loss: 0.4059 - val_acc: 0.8141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b74550710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_train, target_train_small, batch_size=200, epochs=1, validation_data=(sequences_valid, target_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
